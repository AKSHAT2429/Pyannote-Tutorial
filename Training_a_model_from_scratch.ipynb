{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File finder attribute to locate file outside current folders too\n",
    "#using torchaudio.info in order to speed up the data-loading and training procedure\n",
    "import torch, torchmetrics\n",
    "from pyannote.database import FileFinder\n",
    "from pyannote.audio.core.io import get_torchaudio_info\n",
    "preprocessors = {'audio': FileFinder(), \"torchaudio.info\": get_torchaudio_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up database for training\n",
    "\n",
    "from pyannote.database import get_protocol\n",
    "ami = get_protocol('AMI.SpeakerDiarization.only_words',preprocessors=preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use ami.train() to get trainng data protocol and the same for ami.validation() and ami.test()\n",
    "#The code below will provide torchaudio info about each of the audio in the dataset\n",
    "\n",
    "for i in ami.train():\n",
    "    print(get_torchaudio_info(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a voice activity detection model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activity detection (VAD) is the task of detecting speech regions in a given audio stream or recording.\n",
    "\n",
    "We initialize a VAD *task* that describes how the model will be trained:\n",
    "\n",
    "* `ami` indicates that we will use files available in `ami.train()`.\n",
    "* `duration=2.` and `batch_size=128` indicates that the model will ingest batches of 128 two seconds long audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imp: While training on NSCC you need to reset number of workers to 5 to work according to NSCC memory allocation\n",
    "#Note that we can add any number of metrics on which we want our model to trained in the format mentioned below\n",
    "\n",
    "from pyannote.audio.tasks import VoiceActivityDetection\n",
    "vad_task = VoiceActivityDetection(ami, duration=2.0, batch_size=128,num_workers=5,metric=(torchmetrics.F1Score(threshold=0.5, average='micro'),torchmetrics.AUROC(num_classes=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize one *model* with the `PyanNet` architecture used [in that paper](https://arxiv.org/abs/2104.04045).  \n",
    "In particular, we increase the default stride of the initial `sincnet` feature extraction layer to `10`.\n",
    "\n",
    "The model is also provided with the task (`task=vad_task`) for which it is being trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up a vad model according to custom VAD task initialzed above\n",
    "\n",
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "vad_model = PyanNet(task=vad_task, sincnet={'stride': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need to change/update learning rate, weight decay or learning rate schedulers, \n",
    "#you can do so by the below mentioned function\n",
    "\n",
    "def configure_optimizers(model):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,eps= 1e-08,maximize= False,weight_decay=0)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 20)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have changed the parameters, update the parameters for your model\n",
    "\n",
    "from types import MethodType\n",
    "vad_model.configure_optimizers = MethodType(configure_optimizers, vad_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's train with `pytorch-ligthning`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=1)\n",
    "trainer.fit(vad_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained, the model can be applied to a test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use a test file provided by the protocol, but it could be any audio file\n",
    "# e.g. test_file = \"/path/to/test.wav\".\n",
    "\n",
    "test_file = next(ami.test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model was trained on 2s audio chunks and that test files are likely to be much longer than that, we wrap the `model` with an `Inference` instance: it will take care of sliding a 2s window over the whole file and aggregate the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Inference\n",
    "vad = Inference(vad_model)\n",
    "\n",
    "vad_probability = vad(test_file)\n",
    "vad_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect voice activity detection output should look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = test_file[\"annotation\"].get_timeline().support()\n",
    "expected_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyannote]",
   "language": "python",
   "name": "conda-env-pyannote-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
