{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File finder attribute to locate file outside current folders too\n",
    "#using torchaudio.info in order to speed up the data-loading and training procedure\n",
    "import torch, torchmetrics\n",
    "from pyannote.database import FileFinder\n",
    "from pyannote.audio.core.io import get_torchaudio_info\n",
    "preprocessors = {'audio': FileFinder(), \"torchaudio.info\": get_torchaudio_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up database for training\n",
    "\n",
    "from pyannote.database import get_protocol\n",
    "ami = get_protocol('AMI.SpeakerDiarization.only_words',preprocessors=preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#login into huggingface_hub with access token\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "pretrained = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how it performs on our test file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use a test file provided by the protocol, but it could be any audio file\n",
    "# e.g. test_file = \"/path/to/test.wav\".\n",
    "\n",
    "test_file = next(ami.test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Inference\n",
    "spk_probability = Inference(pretrained, step=2.5)(test_file)\n",
    "spk_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perfect output would look like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file[\"annotation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to fine-tune this pretrained model on the AMI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.tasks import Segmentation\n",
    "seg_task = Segmentation(ami, duration=5.0, max_num_speakers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that fine-tuning was actually helpful, we need to evaluate the performance of the pretrained model, and compute the average local diarization error rate on a 5s window sliding over the whole test set. To do so, we need to create a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, protocol, subset=\"test\"):\n",
    "    from pyannote.audio.utils.signal import binarize\n",
    "    from pyannote.audio.utils.metric import DiscreteDiarizationErrorRate\n",
    "    from pyannote.audio.pipelines.utils import get_devices\n",
    "\n",
    "    (device,) = get_devices(needs=1)\n",
    "    metric = DiscreteDiarizationErrorRate()\n",
    "    files = list(getattr(protocol, subset)())\n",
    "\n",
    "    inference = Inference(model, device=device)\n",
    "\n",
    "    for file in files:\n",
    "        reference = file[\"annotation\"]\n",
    "        hypothesis = binarize(inference(file))\n",
    "        uem = file[\"annotated\"]\n",
    "        _ = metric(reference, hypothesis, uem=uem)\n",
    "        \n",
    "    return abs(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate the model and see its local DER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_pretrained = test(model=pretrained, protocol=ami, subset=\"test\")\n",
    "print(f\"Local DER (pretrained) = {der_pretrained * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the model for fine-tuning, simply by overriding its `task` attribute..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "finetuned = deepcopy(pretrained)\n",
    "finetuned.task = seg_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you need to change/update learning rate, weight decay or learning rate schedulers, \n",
    "#you can do so by the below mentioned function\n",
    "\n",
    "def configure_optimizers(model):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,eps= 1e-08,maximize= False,weight_decay=0)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 20)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have changed the parameters, update the parameters for your model\n",
    "\n",
    "from types import MethodType\n",
    "finetuned.configure_optimizers = MethodType(configure_optimizers, finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we train it (for just one epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=1)\n",
    "trainer.fit(finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the performance of the fine-tuned model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_finetuned = test(model=finetuned, protocol=ami, subset=\"test\")\n",
    "print(f\"Local DER (finetuned) = {der_finetuned * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to gradually unfreeze the layers follow this\n",
    "\n",
    "from pyannote.audio.core.callback import GraduallyUnfreeze\n",
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1,max_epochs=500)\n",
    "#Gradually unfreeze layers after every 20 epochs\n",
    "trainer.callbacks.append(GraduallyUnfreeze(epochs_per_stage=20))\n",
    "trainer.fit(finetuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
