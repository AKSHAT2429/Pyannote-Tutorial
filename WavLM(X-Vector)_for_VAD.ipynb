{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd25661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n2202857e/anaconda3/envs/speechbrain/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from MakeSlices.AudioSlicer import AudioSlicer\n",
    "\n",
    "def slicer(path:str):\n",
    "\taudio_file = Path(path)\n",
    "\taudio_list = []\n",
    "\n",
    "\tif 'mp3' in audio_file.suffix:\n",
    "\t\tfrom MakeSlices.PydubSlices import MakeSlices\n",
    "\t\tfrom MakeSlices.PydubSlices import SliceLoader\n",
    "\n",
    "\t\tSlicesKwargs = {'winlen' : 1000, 'stride' : 500,}\n",
    "\t\tLoaderKwargs = {'samplerate' : None, 'to_torch' : True}\n",
    "\n",
    "\telse:\n",
    "\t\tfrom MakeSlices.LibrosaSlices import MakeSlices\n",
    "\t\tfrom MakeSlices.LibrosaSlices import SliceLoader\n",
    "\n",
    "\t\tSlicesKwargs = {'winlen' : 0.5, 'stride' : 0.5,}\n",
    "\t\tLoaderKwargs = {'samplerate' : None, 'to_torch' : True}\n",
    "\n",
    "\n",
    "\tload_audio = AudioSlicer(MakeSlices = MakeSlices,\n",
    "\t\t\t\tSliceLoader = SliceLoader,\n",
    "\t\t\t\tSlicesKwargs = SlicesKwargs,\n",
    "\t\t\t\tLoaderKwargs = LoaderKwargs)\n",
    "\t\n",
    "\taudio_slices = load_audio(audio_file)\t\n",
    "\tfor i, audio_slice in enumerate(audio_slices):\n",
    "# \t\tprint(i, audio_slice.shape)\n",
    "\t\taudio_list.append(audio_slice)\n",
    "\n",
    "\treturn audio_list\n",
    "\n",
    "# audio_slices = slicer('/home/n2202857e/Documents/pyannote-audio/data/pyannote/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44320a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n2202857e/anaconda3/envs/speechbrain/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "def umap_projection(data : np.array, n_components : int = 2):\n",
    "    \n",
    "\n",
    "    ''' data: Numpy array (num_samples X feature_dim) '''\n",
    "\n",
    "\n",
    "    reducer = umap.UMAP(n_components = n_components)\n",
    "\n",
    "    projections = reducer.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9618cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(data, sample_rate=16000, min_utterance_length_ms=25, merge_dist_ms=32,\n",
    "             silence_value=0, silence_rtol=1e-1, silence_atol=1e-2):\n",
    "    import numpy as np\n",
    "    from numpy import ma\n",
    "\n",
    "\n",
    "\n",
    "    min_utterance_length = min_utterance_length_ms * sample_rate / 1000.0\n",
    "    merge_dist = merge_dist_ms * sample_rate / 1000.0\n",
    "\n",
    "\n",
    "\n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = np.array(data)\n",
    "        assert len(data.shape) == 1\n",
    "    \n",
    "    masked = (~ma.getmaskarray(ma.masked_values(data, silence_value, rtol=silence_rtol, atol=silence_atol))).astype(np.int8)\n",
    "    last_i = -1\n",
    "    for i in range(1, masked.shape[0]):\n",
    "        if masked[i - 1] == 1 and masked[i] == 0:\n",
    "            last_i = i - 1\n",
    "        elif masked[i - 1] == 0 and masked[i] == 1 and last_i > -1 and i - last_i <= merge_dist:\n",
    "            masked[last_i:i] = 1\n",
    "            last_i = -1\n",
    "    last_i = -1\n",
    "    for i in range(masked.shape[0]):\n",
    "        if last_i == -1 or (masked[i - 1] == 0 and masked[i] == 1):\n",
    "            last_i = i - 1\n",
    "        elif masked[i - 1] == 1 and masked[i] == 0 and i - last_i < min_utterance_length:\n",
    "            masked[last_i:i] = 0\n",
    "            last_i = -1\n",
    "\n",
    "\n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3f60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_emb_label(emb,label):\n",
    "    plt.scatter(emb[:, 0],emb[:, 1],s=10,c =label)\n",
    "    plt.show()\n",
    "def show_emb_nolabel(emb):\n",
    "    plt.scatter(emb[:, 0],emb[:, 1],s=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b5a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityElement(arr, n) :\n",
    "     \n",
    "    # sort the array in O(nlogn)\n",
    "    arr.sort()  \n",
    "    count, max_ele, temp, f = 1, -1, arr[0], 0\n",
    "    for i in range(1, n) :\n",
    "         \n",
    "        # increases the count if the same element occurs\n",
    "        # otherwise starts counting new element\n",
    "        if(temp == arr[i]) :\n",
    "            count += 1\n",
    "        else :\n",
    "            count = 1\n",
    "            temp = arr[i]\n",
    "             \n",
    "        # sets maximum count\n",
    "        # and stores maximum occurred element so far\n",
    "        # if maximum count becomes greater than n/2\n",
    "        # it breaks out setting the flag\n",
    "        if(max_ele < count) :\n",
    "            max_ele = count\n",
    "            ele = arr[i]\n",
    "             \n",
    "            if(max_ele > (n//2)) :\n",
    "                f = 1\n",
    "                break\n",
    "             \n",
    "    # returns maximum occurred element\n",
    "    # if there is no such element, returns -1\n",
    "    if f == 1 :\n",
    "        return ele\n",
    "    else :\n",
    "        return max_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42203af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "import torch, torchaudio\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus-sv\")\n",
    "model = WavLMForXVector.from_pretrained(\"microsoft/wavlm-base-plus-sv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64245d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory = '/home/n2202857e/Documents/pyannote-audio/data/pyannote/amicorpus'\n",
    "\n",
    "audio_list = []\n",
    "metadata = []\n",
    "count = 0\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename,\"audio\",filename+'.Mix-Headset.wav')\n",
    "    \n",
    "    metadata.append(torchaudio.info(f))\n",
    "    audio_list.append(slicer(f))\n",
    "    \n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    if (count==10):\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "31859106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10501"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f02ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35339094]) <class 'torch.Tensor'> AudioMetaData(sample_rate=16000, num_frames=35339094, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
     ]
    }
   ],
   "source": [
    "# waveform, sample_rate = torchaudio.load('/home/n2202857e/Documents/pyannote-audio/data/pyannote/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav')\n",
    "# metadata = torchaudio.info('/home/n2202857e/Documents/pyannote-audio/data/pyannote/amicorpus/ES2012c/audio/ES2012c.Mix-Headset.wav')\n",
    "# print(waveform[0].shape, type(waveform),metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a457da88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "directory = '/home/n2202857e/Documents/pyannote-audio/data/only_words/rttms/train'\n",
    "\n",
    "sliced_mask_list = []\n",
    "\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    f = os.path.join(directory, filename)\n",
    "    \n",
    "    rttm = pd.read_csv(f,sep=' ',header=None)\n",
    "    rttm.columns=['label_type', 'utt_id', 'channel', 'start', 'duration', 'x', 'y', 'spk_id', 'z','v']\n",
    "    \n",
    "    mask = np.zeros((metadata[count].num_frames))\n",
    "    sampling_rate = metadata[count].sample_rate\n",
    "    for i in range(len(rttm)):\n",
    "        mask[int(rttm['start'][i]*sampling_rate):int((rttm['start'][i]+rttm['duration'][i])*sampling_rate)] = 1\n",
    "\n",
    "    sliced_mask = []\n",
    "    num_frames_per_slice = 8000\n",
    "    for i in range(int(metadata[count].num_frames/num_frames_per_slice)):\n",
    "        sliced_mask.append(majorityElement(mask[i*num_frames_per_slice:(i+1)*num_frames_per_slice],num_frames_per_slice))\n",
    "    sliced_mask.append(0)\n",
    "    \n",
    "    sliced_mask_list.append(sliced_mask)\n",
    "    \n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    if (count==10):\n",
    "        break\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce7b732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[0].sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab2302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# rttm = pd.read_csv('/home/n2202857e/Documents/pyannote-audio/data/only_words/rttms/train/ES2012c.rttm',sep=' ',header=None)\n",
    "# rttm.columns=['label_type', 'utt_id', 'channel', 'start', 'duration', 'x', 'y', 'spk_id', 'z','v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b688d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69701f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = np.zeros((metadata.num_frames))\n",
    "# sampling_rate = metadata.sample_rate\n",
    "# for i in range(len(rttm)):\n",
    "#     mask[int(rttm['start'][i]*sampling_rate):int((rttm['start'][i]+rttm['duration'][i])*sampling_rate)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f69831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliced_mask = []\n",
    "# for i in range(int(metadata.num_frames/8000)):\n",
    "#     sliced_mask.append(majorityElement(mask[i*8000:(i+1)*8000],8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e904edc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# for i in sliced_mask:\n",
    "#     if i==2000:\n",
    "#         count = count+1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa9880cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sliced_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "c559d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(audio_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9247201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the resulting embeddings can be used for cosine similarity-based retrieval\n",
    "# cosine_sim = torch.nn.CosineSimilarity(dim=-1)\n",
    "# similarity = cosine_sim(embeddings[0], embeddings[1])\n",
    "# threshold = 0.7  # the optimal threshold is dataset-dependent\n",
    "# if similarity < threshold:\n",
    "#     print(\"Speakers are not the same!\")\n",
    "# round(similarity.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7593d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f76b9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = torch.zeros(8000)\n",
    "silences = []\n",
    "silences.append(silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "de12c3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0.,  ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "977f65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(audio_list)):\n",
    "#     print(len(j))\n",
    "\n",
    "    np.random.seed(i)\n",
    "    a = np.random.choice(len(audio_list[i]), size=int(len(audio_list[i])*0.8))\n",
    "    \n",
    "    audio_list[i] = np.insert(audio_list[i], a, silences)\n",
    "    sliced_mask_list[i] = np.insert(sliced_mask_list[i], a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0461fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(audio_list[i])==len(sliced_mask_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992095d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/n2202857e/anaconda3/envs/speechbrain/lib/python3.9/site-packages/transformers/models/wavlm/modeling_wavlm.py:1809: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  return (input_length - kernel_size) // stride + 1\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "embeddings_list = []\n",
    "for audio_slices in audio_list:\n",
    "    final_emb = []\n",
    "    for i in range(len(audio_slices)):    \n",
    "        inputs = feature_extractor(audio_slices[i], sampling_rate=16000, return_tensors=\"pt\",padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = model(**inputs).embeddings\n",
    "\n",
    "        last_hidden_states = outputs.hidden_states\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=-1).cpu()\n",
    "        final_emb.append(embeddings)\n",
    "    embeddings_list.append(final_emb)\n",
    "    \n",
    "    count = count +1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59bdb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_embedding = []\n",
    "count = 0 \n",
    "\n",
    "for embeddings in embeddings_list:\n",
    "    transformed_embedding.append([t[0].numpy() for t in embeddings])\n",
    "    show_emb_label(umap_projection(transformed_embedding[count]),sliced_mask_list[count])\n",
    "    \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b7c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_silences_list = []\n",
    "num_frames_per_slice = 8000\n",
    "\n",
    "\n",
    "for i in metadata:\n",
    "    num_silences_list.append(int(0.8*(i.num_frames)/num_frames_per_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70ab8bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchaudio.backend.common.AudioMetaData at 0x7f40d5d6d430>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "554aaa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6904"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transformed_embedding[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd0e16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.66830686e-03, -2.50686556e-02, -2.32856013e-02,  3.22603732e-02,\n",
       "       -1.54198008e-02, -8.02203789e-02, -4.53274809e-02,  3.36432736e-03,\n",
       "       -3.27243879e-02, -2.47094315e-02, -1.08638518e-02, -2.31245533e-02,\n",
       "        2.77865049e-03, -2.56632194e-02, -1.03143707e-01, -4.88215964e-03,\n",
       "       -2.18123645e-02, -2.50225868e-02, -3.00547350e-02, -4.30534668e-02,\n",
       "       -1.22429198e-02, -3.50289829e-02, -2.98339017e-02,  6.14097156e-03,\n",
       "       -1.03674401e-02, -2.54389290e-02, -1.07974345e-02, -1.58925913e-02,\n",
       "       -1.14500746e-02, -7.45474845e-02, -2.94441469e-02, -1.36640649e-02,\n",
       "       -5.05592208e-03, -1.47956293e-02, -2.89838482e-03, -4.43647653e-02,\n",
       "       -7.39014223e-02, -4.45848331e-02, -1.63345002e-02, -3.26130278e-02,\n",
       "       -9.50325374e-03, -1.54562304e-02, -3.86627689e-02, -6.72732294e-03,\n",
       "       -3.90293300e-02, -2.51860693e-02, -8.19451455e-03, -3.82768409e-03,\n",
       "       -4.63133864e-02, -9.82267899e-04, -3.24181281e-02, -2.83361338e-02,\n",
       "       -8.69179261e-04, -3.40997539e-02, -5.83106354e-02, -6.38198713e-03,\n",
       "        2.76850583e-03, -2.21814718e-02, -2.23360434e-02, -2.57431138e-02,\n",
       "       -1.07734827e-02,  5.40282857e-03, -1.30077034e-01, -2.74184048e-02,\n",
       "       -8.51447433e-02, -2.02902104e-03, -7.99685717e-02, -1.71378963e-02,\n",
       "       -1.93258096e-02,  7.00065643e-02, -9.21456423e-03, -1.89687714e-01,\n",
       "       -2.51918454e-02, -1.52461734e-02, -1.30555648e-02, -8.54591355e-02,\n",
       "       -4.39577587e-02, -5.64982602e-03, -2.77324058e-02, -3.45313549e-02,\n",
       "       -1.97787676e-02, -1.72543749e-02, -4.21438552e-02, -7.18397275e-02,\n",
       "       -4.95540723e-03, -8.81244317e-02, -2.92855073e-02, -1.99036449e-02,\n",
       "        2.06230977e-03, -2.03899853e-02, -2.95404773e-02, -6.25776723e-02,\n",
       "       -1.80541396e-01, -1.65489279e-02, -1.37438509e-03,  3.85839317e-04,\n",
       "       -1.11499736e-02,  7.24038517e-04,  2.32548267e-02, -6.55488744e-02,\n",
       "       -2.03984603e-02, -1.47406876e-01, -3.23336311e-02,  1.00266254e-02,\n",
       "       -2.88323183e-02, -5.19992970e-02, -1.98920611e-02, -1.02638723e-02,\n",
       "        9.55155492e-02, -4.77705486e-02, -1.31582916e-02, -2.20848825e-02,\n",
       "       -3.67305428e-03, -1.81419756e-02, -2.88905781e-02, -3.81037220e-02,\n",
       "        1.57404803e-02, -6.34800717e-02, -7.78710330e-03,  4.75734733e-02,\n",
       "       -2.92313658e-02, -1.07774716e-02, -2.03207904e-03, -2.28099525e-02,\n",
       "        3.68403969e-03, -2.01033391e-02, -6.28019799e-04, -6.61637075e-03,\n",
       "       -5.95289096e-03, -2.10260339e-02, -6.34715194e-03, -8.50739554e-02,\n",
       "       -1.47056943e-02, -1.01928366e-02,  6.95849396e-03, -1.65142715e-02,\n",
       "       -1.54778250e-02, -2.84866337e-02, -9.16259177e-03, -8.56126845e-02,\n",
       "       -2.41267611e-03,  1.91750267e-04,  3.14809978e-02, -4.10181582e-02,\n",
       "       -1.25830583e-02,  4.23629321e-02, -5.22213280e-02, -1.78547073e-02,\n",
       "       -1.05153886e-03, -2.67767254e-02,  5.75102828e-02, -1.22817934e-01,\n",
       "       -1.42745189e-02, -2.17977390e-02, -6.99492870e-03, -3.10467798e-02,\n",
       "       -7.50167354e-04, -3.02856080e-02, -3.08215851e-03, -2.29606051e-02,\n",
       "       -8.29963014e-03, -1.47445649e-02, -9.37093049e-03, -8.46370775e-03,\n",
       "       -5.66632710e-02,  3.86463553e-02,  2.03774590e-02, -1.88127961e-02,\n",
       "       -2.09066886e-02, -1.51904235e-02, -2.51637027e-02, -1.08840138e-01,\n",
       "        4.72988971e-02, -6.33396953e-02, -2.64071785e-02,  2.29552183e-02,\n",
       "       -1.01164286e-03, -2.40582507e-02, -7.07575632e-03, -1.94577873e-02,\n",
       "       -5.39821312e-02, -2.14195885e-02, -2.00716574e-02, -1.16114188e-02,\n",
       "       -1.21043157e-02, -5.55212870e-02, -2.04860941e-01, -2.79528797e-02,\n",
       "       -2.38806177e-02, -3.01995091e-02, -7.77202100e-03, -2.91441604e-02,\n",
       "        3.52369323e-02, -4.38869465e-03,  3.12197693e-02, -2.75367107e-02,\n",
       "        3.70210633e-02, -1.97282601e-02,  1.89103797e-04, -4.35053371e-03,\n",
       "       -5.96390525e-03, -4.08787001e-03,  6.08302504e-02, -2.35007033e-02,\n",
       "        1.26235047e-03, -1.97667256e-02, -4.96039018e-02, -2.29161102e-02,\n",
       "       -2.69398540e-02,  4.61180322e-02,  2.42520832e-02, -2.16580592e-02,\n",
       "       -3.80940996e-02, -1.91068761e-02, -2.19366644e-02, -4.53734882e-02,\n",
       "       -9.91035439e-03, -5.10833273e-03, -6.19472563e-02,  4.81512211e-02,\n",
       "       -1.01191346e-02, -1.00730486e-01, -3.19821127e-02, -2.23820657e-02,\n",
       "        1.04431389e-02, -8.02516863e-02, -2.46391334e-02,  3.62317115e-02,\n",
       "       -2.97999159e-02,  3.64059135e-02, -3.01376339e-02,  1.28948437e-02,\n",
       "       -1.46743888e-02, -2.45714169e-02, -3.32109146e-02, -2.69657169e-02,\n",
       "       -1.51062738e-02, -1.62931569e-02, -7.02826828e-02, -2.47087982e-02,\n",
       "       -1.76835507e-02, -2.90167164e-02, -2.02112179e-03, -7.51727377e-04,\n",
       "       -3.59946191e-02, -8.81452784e-02, -3.28632891e-02, -1.76826060e-01,\n",
       "       -1.50616234e-02, -7.20665092e-03,  4.93694609e-03, -2.14551613e-02,\n",
       "       -2.84292754e-02, -1.79036111e-02, -1.52086802e-02, -8.44957831e-04,\n",
       "       -6.95771025e-03, -2.62854323e-02, -9.94174406e-02, -2.65828911e-02,\n",
       "        8.80383421e-03, -2.02146787e-02, -1.36541771e-02, -2.53287721e-02,\n",
       "        9.34794731e-03, -3.46010365e-02, -6.84544593e-02, -1.84093509e-02,\n",
       "        4.32836683e-03,  7.78398244e-03, -3.33489366e-02, -1.72211863e-02,\n",
       "       -1.06994614e-01, -2.47498341e-02, -1.81457344e-02, -2.48115174e-02,\n",
       "        2.53564268e-02, -7.31561854e-02, -9.34085622e-03,  7.16985241e-02,\n",
       "       -2.76153237e-02, -6.42117020e-03, -6.90488890e-02, -4.24554013e-02,\n",
       "       -6.15259483e-02, -1.91233717e-02,  2.13878937e-02, -1.86262410e-02,\n",
       "       -1.40974317e-02, -2.93316208e-02, -1.47170189e-03, -1.58815123e-02,\n",
       "       -7.19414651e-03,  2.57195979e-02, -3.15441079e-02, -2.63710390e-03,\n",
       "       -5.54698929e-02, -3.45895700e-02,  6.85429573e-02, -2.44070925e-02,\n",
       "       -4.50137910e-03, -2.41754930e-02, -2.52544899e-02, -1.44876530e-02,\n",
       "       -6.90819323e-03, -4.68852110e-02,  7.48740742e-04, -2.06896756e-02,\n",
       "       -1.77771170e-02, -2.33777594e-02,  8.16262444e-04, -8.53572506e-03,\n",
       "       -1.28442682e-02, -8.32668040e-03, -3.55888456e-02, -1.42168224e-01,\n",
       "       -5.25327325e-02, -2.78848149e-02, -7.45467143e-03, -2.62407102e-02,\n",
       "       -6.17814902e-03, -3.96667011e-02,  3.77579927e-02, -1.10340929e-02,\n",
       "       -2.61130631e-02, -1.10677322e-02, -3.48105617e-02, -1.88376810e-02,\n",
       "       -1.80857312e-02, -9.84073337e-03, -2.01595519e-02, -2.43424810e-02,\n",
       "        4.68572006e-02, -1.87705830e-02,  5.90046216e-03, -4.12004441e-02,\n",
       "       -1.70586426e-02, -1.30305901e-01, -2.41058487e-02, -1.20392879e-02,\n",
       "        4.04766053e-02, -9.96570662e-02, -2.16828492e-02, -1.31111182e-02,\n",
       "       -4.58407924e-02, -1.25661138e-02, -6.40828162e-02, -2.06010882e-02,\n",
       "        1.46042677e-02,  2.20288318e-02, -3.04504801e-02, -3.17349061e-02,\n",
       "       -4.51006293e-02, -7.00947829e-03, -2.69502029e-02, -2.11111344e-02,\n",
       "       -2.39427071e-02, -1.75412763e-02, -5.91509463e-03, -1.01747969e-02,\n",
       "       -1.27107475e-03, -3.92431207e-02, -2.37047430e-02, -8.82884208e-03,\n",
       "       -6.77706450e-02, -7.65837403e-03, -3.61426510e-02, -5.41480072e-02,\n",
       "       -2.66415570e-02, -7.07736909e-02,  3.30096157e-03, -2.52976026e-02,\n",
       "       -7.44362548e-03, -1.07411683e-01, -5.32238074e-02, -2.34835017e-02,\n",
       "        8.66431966e-02, -9.96115617e-03, -2.28964034e-02, -3.49975377e-02,\n",
       "       -1.84890106e-02, -3.34498584e-02, -1.87343173e-02, -2.67358143e-02,\n",
       "        3.73469898e-04, -2.45671999e-02, -2.26038303e-02, -3.40022817e-02,\n",
       "       -3.00362688e-02, -2.25249186e-01, -4.49242853e-02, -1.43053057e-02,\n",
       "       -2.82278936e-02, -2.51611788e-02, -5.63855283e-03, -3.48553695e-02,\n",
       "        9.37685817e-02, -5.67097822e-03, -4.20072526e-02, -1.95203368e-02,\n",
       "       -1.65392570e-02, -1.37765929e-02, -4.06681113e-02,  1.14980212e-03,\n",
       "       -3.97460610e-02, -1.47600984e-02, -1.23091759e-02, -2.91435495e-02,\n",
       "       -7.42041394e-02, -3.68629619e-02, -9.32889953e-02, -2.21352652e-02,\n",
       "       -1.34393070e-02, -2.47323979e-02, -1.85898098e-03, -2.55207717e-02,\n",
       "       -1.17064761e-02, -3.54358517e-02, -9.02888477e-02, -6.24067597e-02,\n",
       "       -2.26421077e-02, -3.93660441e-02, -8.19860026e-02, -5.79402447e-02,\n",
       "       -2.61541866e-02, -8.78657214e-03, -2.07000114e-02, -3.08355689e-02,\n",
       "       -1.99148189e-02, -2.32450813e-02, -1.10895343e-01, -2.59044953e-02,\n",
       "       -6.08388223e-02, -2.75786016e-02, -1.10155242e-02, -6.81622699e-02,\n",
       "       -3.09569389e-02, -2.11681109e-02, -5.55706844e-02,  1.79344532e-03,\n",
       "        2.55826283e-02,  5.99407125e-03, -1.23480611e-01, -2.53855176e-02,\n",
       "       -1.22313050e-03, -2.24494115e-02, -1.55940484e-02, -1.32734161e-02,\n",
       "        2.35089008e-02,  2.29826011e-02, -1.08488448e-01,  2.55538933e-02,\n",
       "       -6.54606009e-03, -4.47328994e-03,  8.51887614e-02, -9.58502758e-03,\n",
       "       -4.56135310e-02, -9.62010548e-02, -3.34382653e-02, -2.47141235e-02,\n",
       "       -1.98698584e-02, -1.07792914e-01,  3.31638753e-02,  5.51791722e-03,\n",
       "       -1.06195502e-01,  1.50043098e-02,  2.56523746e-03, -1.18164038e-02,\n",
       "       -3.41472849e-02, -4.30336222e-02, -2.25368533e-02,  2.19238480e-03,\n",
       "       -1.25159072e-02, -3.24420296e-02, -4.56200577e-02, -2.45325826e-03,\n",
       "       -1.63443498e-02, -2.35494133e-03, -1.47061840e-01, -3.07362508e-02,\n",
       "       -7.31038749e-02, -3.26466747e-02, -2.43192390e-02, -8.15450400e-02,\n",
       "       -1.19130441e-03, -4.56295051e-02, -2.13563982e-02, -3.05394996e-02,\n",
       "        1.10565364e-01, -1.89817939e-02,  5.11175059e-02, -4.95122857e-02,\n",
       "       -1.65272783e-02, -2.36903392e-02,  2.89612673e-02, -4.18048352e-02,\n",
       "       -2.62222923e-02, -1.27545716e-02,  1.23516191e-02, -3.37328389e-02,\n",
       "       -1.99155342e-02, -2.11061090e-02, -1.07903769e-02, -1.03955716e-01,\n",
       "       -1.82474237e-02, -3.19076553e-02, -2.53792875e-03, -3.63268144e-02,\n",
       "       -2.19677878e-03, -3.64453867e-02, -1.89821899e-03, -7.13211577e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_embedding[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f62f9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = np.zeros((512,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bd4ed81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a566fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,j in enumerate(transformed_embedding):\n",
    "#     transformed_embedding[i] = np.insert(j, np.random.choice(len(j), size=int(len(j)*0.8)), silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f602ac6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (8000,) could not be broadcast to indexing result of shape (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m20\u001b[39m)))\n\u001b[1;32m      2\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 3\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m a\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/speechbrain/lib/python3.9/site-packages/numpy/lib/function_base.py:5377\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5375\u001b[0m slobj[axis] \u001b[38;5;241m=\u001b[39m indices\n\u001b[1;32m   5376\u001b[0m slobj2[axis] \u001b[38;5;241m=\u001b[39m old_mask\n\u001b[0;32m-> 5377\u001b[0m new[\u001b[38;5;28mtuple\u001b[39m(slobj)] \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m   5378\u001b[0m new[\u001b[38;5;28mtuple\u001b[39m(slobj2)] \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m   5380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (8000,) could not be broadcast to indexing result of shape (5,)"
     ]
    }
   ],
   "source": [
    "a = list(np.ones((20)))\n",
    "n = 5\n",
    "a = np.insert(a, np.random.choice(20, size=n), silence)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb803fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = [t[0].numpy() for t in final_emb]\n",
    "# show_emb_label(umap_projection(new),sliced_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "49907c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import speechbrain as sb\n",
    "\n",
    "class SimpleBrain(sb.Brain):\n",
    "    def compute_forward(self, batch, stage):\n",
    "        return self.modules.model(batch[\"input\"])\n",
    "\n",
    "    \n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "#         return torch.nn.functional.l1_loss(predictions, batch[\"target\"])\n",
    "        return sb.nnet.losses.bce_loss(predictions, batch[\"target\"])\n",
    "\n",
    "#model = torch.nn.Sequential(torch.nn.LSTM(input_size = 512,hidden_size=128,num_layers=4,dropout =0.5,bidirectional=True),torch.nn.Linear(128,128),torch.nn.Softmax(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "60a6288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.bilstm = torch.nn.LSTM(input_size = 512,hidden_size=64,num_layers=4,dropout =0.5,bidirectional=True)#,batch_first=True)\n",
    "        self.linear1 = torch.nn.Linear(128,128)\n",
    "        self.linear2 = torch.nn.Linear(128,2)\n",
    "#         self.linear_relu_stack = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(28*28, 512),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(512, 512),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(512, 10),\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x,y = self.bilstm(x)\n",
    "        out = self.linear1(x)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "6d3f7367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (bilstm): LSTM(512, 64, num_layers=4, dropout=0.5, bidirectional=True)\n",
      "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=2)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d5115845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_torch = torch.from_numpy(np.array(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "059d69e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4418, 512])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a30b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_mask_torch = torch.from_numpy(np.array(sliced_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6c2e81c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliced_mask_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "53b4db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_mask_torch=sliced_mask_torch.reshape(4418)\n",
    "\n",
    "import torch.nn.functional as Fun\n",
    "one_hot = Fun.one_hot(sliced_mask_torch.to(torch.int64), num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ec8309b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "0535d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4418, 512])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "ee6e9e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4418, 2])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9acd72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = []\n",
    "for i in range(64):\n",
    "    for j in range(int(4418/64)):\n",
    "        batch_torch.append(new_torch[j*64:(j+1)*64])\n",
    "        dict1 = {\"input\": new_torch[i], \"target\": one_hot[i]}\n",
    "    data_new.append(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "7f2c48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "hparams_file = \"hyp.yaml\"\n",
    "\n",
    "with open(hparams_file) as fin:\n",
    "\n",
    "    hparams = load_hyperpyyaml(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "9afc2dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 70/70 [00:07<00:00,  9.31it/s, train_loss=0.494]\n",
      "100%|| 70/70 [00:07<00:00,  9.22it/s, train_loss=0.489]\n",
      "100%|| 70/70 [00:08<00:00,  8.74it/s, train_loss=0.5]\n",
      "100%|| 70/70 [00:07<00:00,  9.01it/s, train_loss=0.465]\n",
      "100%|| 70/70 [00:07<00:00,  8.80it/s, train_loss=0.464]\n",
      "100%|| 70/70 [00:07<00:00,  8.92it/s, train_loss=0.506]\n",
      "100%|| 70/70 [00:07<00:00,  9.20it/s, train_loss=0.462]\n",
      "100%|| 70/70 [00:07<00:00,  8.81it/s, train_loss=0.454]\n",
      "100%|| 70/70 [00:08<00:00,  8.38it/s, train_loss=0.43]\n",
      "100%|| 70/70 [00:08<00:00,  8.65it/s, train_loss=0.367]\n"
     ]
    }
   ],
   "source": [
    "brain = SimpleBrain({\"model\": model}, opt_class=lambda x: torch.optim.Adam(x))\n",
    "data = [{\"input\": new_torch, \"target\": one_hot}]\n",
    "brain.fit(range(10), data_new,train_loader_kwargs=hparams[\"dataloader_options\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "dfafa571",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\"input\": new_torch[-10].reshape(1,512)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e626e8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_torch[-2].reshape(1,512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "64821943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "8c81c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([[-0.3809,  0.5143]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = brain.compute_forward(test,stage=3)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "51f7f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3645\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in sliced_mask_torch:\n",
    "    if i == 1:\n",
    "        count =count +1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "12206ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_item_by_idx',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "8499fbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.weight', Parameter containing:\n",
      "tensor([[-0.0330, -0.0186,  0.0369,  ..., -0.0313,  0.0146, -0.0516],\n",
      "        [ 0.0560,  0.0094, -0.0207,  ..., -0.0033,  0.0498,  0.0189],\n",
      "        [-0.0122,  0.0279, -0.0160,  ...,  0.0153, -0.0415, -0.0350],\n",
      "        ...,\n",
      "        [ 0.0605, -0.0034, -0.0140,  ..., -0.0072,  0.0149,  0.0575],\n",
      "        [-0.0037,  0.0501,  0.0012,  ..., -0.0103,  0.0143, -0.0187],\n",
      "        [-0.0139, -0.0418, -0.0313,  ..., -0.0159, -0.0233, -0.0401]],\n",
      "       requires_grad=True))\n",
      "('0.bias', Parameter containing:\n",
      "tensor([ 1.3918e-02,  4.1625e-03,  1.9940e-02,  2.6180e-02,  2.2688e-02,\n",
      "         2.5424e-02, -1.7914e-02, -2.0885e-02, -4.0077e-02, -4.1558e-02,\n",
      "        -1.0527e-02, -6.8387e-03, -4.8117e-02, -1.8956e-02,  3.4427e-02,\n",
      "        -4.6843e-02,  3.4231e-02, -5.4439e-02,  5.2945e-02, -2.6281e-03,\n",
      "         2.5140e-02, -4.4587e-02,  1.3619e-02,  4.0298e-02, -1.9755e-02,\n",
      "        -2.3687e-02, -7.6622e-03,  1.4093e-02, -1.8238e-02, -1.5738e-02,\n",
      "        -1.0142e-02, -1.7558e-02, -5.5290e-02,  3.0359e-03, -2.8419e-02,\n",
      "        -3.1765e-02,  1.3754e-02, -3.8593e-02, -1.0797e-02, -4.9363e-03,\n",
      "        -7.0042e-03, -4.1595e-02,  1.8805e-02,  3.2609e-02, -4.5753e-03,\n",
      "         3.8663e-02, -1.7616e-02, -1.2828e-02,  2.5088e-02,  4.7024e-02,\n",
      "        -2.7274e-02, -1.6848e-02, -2.8563e-02, -2.8279e-02,  2.7694e-02,\n",
      "         4.9798e-02,  4.9293e-02,  3.7130e-02, -4.7998e-03, -5.3956e-02,\n",
      "        -1.9765e-02,  1.7239e-02,  9.1665e-03,  3.3171e-02,  2.7861e-02,\n",
      "        -1.0324e-02, -1.7106e-02,  2.5371e-02, -3.5277e-02,  3.7282e-02,\n",
      "         9.6468e-03, -4.4045e-02,  2.9664e-03, -5.2474e-02,  4.6581e-02,\n",
      "        -4.2439e-02,  3.1093e-02,  1.0338e-02,  2.6615e-02,  3.1616e-02,\n",
      "         4.7059e-02, -3.3335e-03, -1.9477e-02, -1.1158e-02, -5.1493e-02,\n",
      "         1.7145e-02,  2.9318e-02, -2.2520e-02, -4.8420e-02, -1.8395e-02,\n",
      "        -5.6140e-03,  1.4845e-02,  5.4450e-03,  3.2217e-02,  4.8149e-02,\n",
      "         5.8272e-03, -3.6906e-03, -1.4074e-02, -2.9064e-02,  2.9211e-02,\n",
      "         2.2077e-02, -1.1558e-02,  1.6969e-02, -2.9448e-02,  4.0419e-02,\n",
      "         2.0193e-02, -2.7531e-02,  1.7625e-03, -2.0558e-02, -5.1848e-02,\n",
      "        -1.3136e-02, -3.1890e-02, -1.4100e-02, -2.6036e-02,  6.4379e-03,\n",
      "         1.9278e-02,  1.8317e-02,  2.2083e-02, -1.2012e-03, -4.7250e-03,\n",
      "        -4.3545e-02, -3.3689e-02,  2.8348e-02, -1.2764e-02,  1.4184e-02,\n",
      "         3.7596e-02, -2.8264e-02,  3.3535e-02,  2.8815e-02,  1.4638e-02,\n",
      "         1.1860e-02,  3.8151e-02,  4.1818e-02, -1.0520e-02,  3.0277e-02,\n",
      "        -3.6820e-02,  1.5902e-02,  3.3151e-02, -4.4907e-04,  4.4556e-02,\n",
      "         1.5524e-02,  1.6389e-02, -9.0926e-03,  1.1020e-02, -3.1388e-02,\n",
      "         1.5745e-02, -9.9107e-03, -3.4115e-02,  3.4787e-02,  4.1739e-03,\n",
      "         4.7065e-02,  3.3837e-03, -4.2288e-02, -3.4860e-02, -4.3182e-03,\n",
      "         5.3138e-02, -7.5184e-03,  5.9256e-03, -3.3601e-02,  3.3040e-02,\n",
      "        -1.5724e-02, -2.2495e-02, -2.9331e-02, -4.8845e-02, -3.0603e-02,\n",
      "         8.0895e-03, -6.0875e-03, -2.2823e-02,  6.1052e-03,  1.5339e-02,\n",
      "        -1.9180e-02,  2.1564e-02, -2.5798e-02,  9.1093e-03,  5.3601e-02,\n",
      "         8.5010e-03,  2.2910e-02, -9.0489e-03, -1.7282e-02, -4.4209e-02,\n",
      "        -1.9891e-02,  2.4664e-02,  4.4338e-02,  2.8490e-02, -4.9596e-02,\n",
      "         3.0671e-02, -4.4119e-02,  1.5088e-02, -2.1658e-02, -5.2554e-02,\n",
      "        -6.8553e-03,  5.2535e-02,  8.4465e-03, -4.2270e-02, -7.8347e-03,\n",
      "        -1.2139e-02, -2.0541e-03,  1.4901e-02,  4.9635e-03, -2.0198e-02,\n",
      "         3.5976e-02, -2.5905e-02,  3.5479e-02,  1.7063e-02, -5.1910e-02,\n",
      "         2.5151e-02, -5.3899e-02,  4.1035e-02,  9.6178e-05,  1.2256e-02,\n",
      "         4.6225e-04, -1.2446e-02, -2.1686e-02,  3.4979e-02, -7.6478e-03,\n",
      "         3.7565e-02, -3.8197e-04, -3.6133e-02, -1.7330e-02, -4.4212e-02,\n",
      "         1.9109e-02,  3.9969e-02, -1.6343e-02,  2.4107e-02,  4.5587e-02,\n",
      "         4.1556e-02,  3.1330e-03,  1.2604e-02, -2.9523e-02, -1.0664e-02,\n",
      "         1.0344e-02, -1.1620e-03, -2.0838e-02, -4.6235e-02, -1.7064e-02,\n",
      "         2.9123e-03,  2.3569e-02, -3.9265e-02, -5.6928e-03,  1.3863e-02,\n",
      "        -8.0928e-03,  3.7546e-02,  4.3570e-02, -5.1252e-02,  4.3185e-02,\n",
      "        -2.2304e-02,  2.8896e-02,  1.5679e-02,  3.0514e-02,  5.3490e-02,\n",
      "         7.8370e-03, -3.1018e-02, -7.4864e-03,  2.8558e-02, -2.9650e-02,\n",
      "        -2.6319e-02, -2.7146e-02, -3.0236e-02,  4.5238e-02, -1.0327e-02,\n",
      "         3.3830e-02,  1.7126e-02, -4.7283e-02, -2.8835e-02,  2.2750e-02,\n",
      "        -1.2033e-02,  1.9104e-02, -3.8088e-02,  1.1764e-02,  4.7212e-02,\n",
      "         1.3023e-02, -2.2650e-02, -1.5415e-02,  7.5739e-04, -2.9835e-02,\n",
      "         3.4085e-02,  3.0088e-04,  4.1386e-02, -2.5809e-02,  4.8663e-02,\n",
      "        -2.3275e-02,  3.7761e-02, -2.1208e-02,  2.7453e-02,  4.6713e-04,\n",
      "        -3.7506e-02, -4.0135e-02,  2.2923e-02,  4.5222e-02, -2.4867e-02,\n",
      "        -1.4996e-02, -2.6764e-02,  5.5549e-03,  1.7219e-02,  1.3661e-03,\n",
      "         1.4338e-02,  3.3804e-02,  1.1977e-02, -4.0004e-02, -3.9223e-02,\n",
      "        -2.0787e-02, -1.9784e-02, -4.3096e-02, -4.2406e-02,  4.4225e-03,\n",
      "         3.3185e-02,  3.5188e-02,  4.2109e-02,  9.3726e-05,  8.7921e-03,\n",
      "         4.0273e-02, -2.8110e-02,  4.1245e-02,  1.3710e-02, -3.5201e-02,\n",
      "         2.8262e-02, -7.6905e-03, -1.2643e-02, -1.0512e-02, -1.4615e-02,\n",
      "        -3.5764e-03,  4.6345e-02,  7.5488e-03, -1.5545e-02,  3.1519e-02,\n",
      "        -7.1333e-03,  3.9692e-02, -1.6110e-02,  2.0480e-02, -2.1252e-02,\n",
      "         3.3023e-02,  1.8889e-02, -4.4949e-03,  4.1217e-02,  2.9400e-02,\n",
      "         5.1017e-02, -8.3911e-03, -2.3271e-02, -1.1165e-02,  3.2362e-03,\n",
      "        -4.0421e-02,  1.6681e-02, -3.6907e-02, -3.4518e-02,  9.4387e-03,\n",
      "         4.8617e-02,  4.9693e-02, -3.9656e-02, -5.2913e-02,  3.0984e-02,\n",
      "        -4.1670e-02,  2.6783e-02,  1.1989e-02, -3.8229e-02,  2.5677e-02,\n",
      "        -3.3252e-02, -2.5429e-02,  1.3866e-02,  3.1305e-04, -1.4469e-02,\n",
      "        -4.7036e-02,  3.6006e-03,  2.6695e-02,  2.4185e-03,  5.0538e-02,\n",
      "         2.4701e-02,  4.7267e-02,  2.6067e-02,  7.2594e-04, -5.1464e-03,\n",
      "        -7.0374e-04,  2.8305e-02, -2.4147e-02,  1.7705e-03,  4.5015e-02,\n",
      "        -1.5737e-02, -1.6187e-03,  1.2734e-02, -1.0244e-02,  4.4385e-03,\n",
      "         1.1905e-02,  4.0406e-02,  2.6972e-02, -1.3528e-02,  6.9285e-03,\n",
      "         3.9846e-02, -8.2221e-03,  4.4829e-02, -1.8530e-02, -3.2981e-02,\n",
      "        -5.1679e-02, -1.5014e-02,  2.4262e-02, -1.9908e-02,  1.8040e-02,\n",
      "         4.8758e-02, -1.2118e-02, -4.9676e-02, -1.4761e-02,  1.9425e-02,\n",
      "         3.2324e-02,  4.2942e-02,  2.8784e-02, -2.3174e-02, -2.5530e-02,\n",
      "         1.9409e-02,  1.9997e-02,  8.7260e-03,  4.7673e-03, -1.5366e-02,\n",
      "         3.4621e-02, -2.4296e-02, -7.9408e-04, -5.3005e-02,  5.3242e-02,\n",
      "         3.3307e-02,  1.5071e-02, -2.3714e-02,  2.6961e-02, -3.9608e-02,\n",
      "         4.2063e-02, -1.8270e-02,  1.0915e-02, -4.1321e-02, -1.1745e-02,\n",
      "        -4.1347e-02,  2.4801e-02, -1.7953e-02, -1.9437e-02, -4.5685e-02,\n",
      "         1.7085e-02,  4.9982e-02, -4.5694e-02, -1.7837e-02, -3.8566e-02,\n",
      "         3.4870e-02, -7.5399e-03,  4.2660e-02,  2.3252e-02, -6.6554e-03,\n",
      "         4.0109e-02,  2.7343e-02,  1.4957e-02, -5.2913e-03,  4.0463e-02,\n",
      "        -3.9067e-02,  1.0186e-02, -4.2083e-03,  4.1855e-03,  1.5808e-02,\n",
      "        -4.2524e-02,  4.1679e-02, -2.4167e-03, -2.4721e-02, -2.0715e-02,\n",
      "         4.0172e-02, -2.8668e-02, -2.5271e-03, -1.9103e-02,  1.4443e-02,\n",
      "        -2.2001e-02, -1.5170e-02, -4.8366e-03,  2.8804e-02, -2.0474e-02,\n",
      "        -1.1154e-02, -1.7210e-02, -5.0556e-02, -5.4473e-03,  3.2585e-02,\n",
      "         7.1250e-03, -4.6295e-02,  4.5361e-02, -2.1384e-02, -5.1012e-03,\n",
      "         1.9994e-02,  8.4107e-03,  1.8443e-02,  3.1019e-03,  2.0605e-02,\n",
      "         2.4943e-02, -4.6686e-02,  2.7088e-02,  2.8079e-03, -1.6526e-02,\n",
      "        -1.4271e-02, -3.8417e-03,  9.0410e-03,  2.6768e-02, -1.6438e-02,\n",
      "         1.7482e-02,  3.3441e-03,  2.7828e-02,  1.6642e-02,  4.1014e-02,\n",
      "        -4.7205e-02, -4.1014e-02, -3.2069e-02, -2.9528e-02,  2.3122e-02,\n",
      "         1.1753e-02, -1.4145e-04,  5.9116e-03,  1.4975e-02,  1.6261e-04,\n",
      "         3.6235e-02,  2.0584e-02, -2.3698e-02,  2.7478e-02, -5.1271e-02,\n",
      "        -7.4250e-04,  2.2852e-03], requires_grad=True))\n",
      "('1.weight', Parameter containing:\n",
      "tensor([[-0.0155,  0.0506, -0.0315,  ...,  0.0072,  0.0490, -0.0491],\n",
      "        [ 0.0475, -0.0435,  0.0442,  ..., -0.0515, -0.0123,  0.0323]],\n",
      "       requires_grad=True))\n",
      "('1.bias', Parameter containing:\n",
      "tensor([-0.0111, -0.0055], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for i in model.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c94608b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:speechbrain]",
   "language": "python",
   "name": "conda-env-speechbrain-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
